{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Xenus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Xenus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "morph_analyzer = MorphAnalyzer()\n",
    "model_ruscorpora = api.load('word2vec-ruscorpora-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word): #приводим слова в нормальную форму\n",
    "    parse = morph_analyzer.parse(word)[0]\n",
    "    return(parse.normal_form+'_'+parse.tag.POS)\n",
    "        \n",
    "def tokenize(text): #разбиваем текст на слова\n",
    "    if type(text) == str:\n",
    "        words = [i for i in word_tokenize(text) if i not in stop_words and not i.isdigit()]\n",
    "        words = [lemmatize(i) for i in words if morph_analyzer.parse(i)[0].tag.POS != None]\n",
    "        return words\n",
    "    else: return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ready_dataset.csv')\n",
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = df.copy() #создаем рабочий экзмепляр данных\n",
    "\n",
    "df_words['words'] = df.msge.apply(tokenize) #токенизируем текст, замещаем в датафрейме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer() #tf-idf\n",
    "dim_reduct = TruncatedSVD(n_components=2) #учим снижать размерность\n",
    "X = vectorizer.fit_transform(df.msge)\n",
    "dim_reduct.fit(X)\n",
    "\n",
    "\n",
    "coords = dim_reduct.transform(X) #снижаем размерность\n",
    "df_words['tf_idf_x'], df_words['tf_idf_y'] = coords[:, 0], coords[:, 1]\n",
    "\n",
    "df_words['tf_idf_x'] = df_words['tf_idf_x']+abs(df_words['tf_idf_x'].min()) #нормализуем векторы\n",
    "df_words['tf_idf_y'] = df_words['tf_idf_y']+abs(df_words['tf_idf_y'].min())\n",
    "df_words['tf_idf_x'] *= 1/df_words['tf_idf_x'].max()\n",
    "df_words['tf_idf_y'] *= 1/df_words['tf_idf_y'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = df_words[df_words.words.apply(len) != 0] #убираем пустые текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_words_arr(list_of_words):\n",
    "    clean_list = []\n",
    "    for i in list_of_words:\n",
    "        clean_list.append(re.sub(r'(_[^_]*$)', '', i))\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_words['words_clean'] = df_words.words.apply(get_clean_words_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка w2v модели\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#модель w2v на данных. Не запускать, если уже имеется\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(df_words['words_clean'], size=500, window=5, min_count=3, workers=4, sg=0, negative=20)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_with_meta = [] #создаем промежуточный список для последующего создания датафрейма\n",
    "for sample in df_words.iterrows(): #идем по датафрейму, выбираем слова, забираем их параметры\n",
    "    for word in sample[1].words:\n",
    "        all_words_with_meta.append([word, sample[1]['date'], sample[1]['time'], sample[1]['user']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_with_meta = np.array(all_words_with_meta) #переводим в массив numpy\n",
    "df_words_sep = pd.DataFrame({'word':all_words_with_meta[:, 0],\n",
    "                        'date':all_words_with_meta[:, 1],\n",
    "                       'time':all_words_with_meta[:, 2],\n",
    "                        'user':all_words_with_meta[:, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_sep['word_clean'] = df_words_sep.word.apply(lambda x: re.sub(r'(_[^_]*$)', '', x)) #чистим слова от частей речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_sep['word_POS_TAG'] = df_words_sep.word.apply(lambda x: re.sub(r'(^[^_]*_)', '', x)) #забираем части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    if word in model.wv.vocab: return model.wv[word]\n",
    "    else: return np.nan\n",
    "def get_word_embedding_ruscorpora(word):\n",
    "    if word in model_ruscorpora.vocab: return model_ruscorpora[word]\n",
    "    else: return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_sep['word_embedding_ruscorpora'] = df_words_sep.word.apply(get_word_embedding_ruscorpora)\n",
    "df_words_sep['word_embedding'] = df_words_sep.word_clean.apply(get_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_words_sep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8634f9205265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_words_sep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ready_dataset_with_word_embeddings.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ready_dataset_messages_tf-idf.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_words_sep' is not defined"
     ]
    }
   ],
   "source": [
    "df_words_sep.to_pickle('ready_dataset_with_word_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
